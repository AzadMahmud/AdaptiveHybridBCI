"""
Updated TCN Training and Evaluation Script for BCI Competition IV Dataset 2a.

This script demonstrates how to use the TCN model with the preprocessed data 
generated by the preprocessing pipeline. It loads real data, trains the TCN, 
and evaluates it. This serves as a baseline for the 'Deep Stream' component 
of the Hybrid BCI.

Usage:
    cd src/models
    python train_tcn_updated.py --subject_id A01 --epochs 10 --batch_size 32
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, cohen_kappa_score
from sklearn.model_selection import train_test_split

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# Import the TCN model
from src.models.tcn import TCN

# --- Configuration ---
DATA_DIR = os.path.join(project_root, 'data', 'preprocessed')
RESULTS_DIR = os.path.join(project_root, 'results', 'tcn')

def load_preprocessed_data(subject_id, session, data_dir=DATA_DIR):
    """
    Loads preprocessed epochs and labels for a specific subject and session.
    
    Args:
        subject_id (str): Subject identifier (e.g., 'A01').
        session (str): Session identifier ('T' for training).
        data_dir (str): Directory containing preprocessed data.
        
    Returns:
        tuple: (epochs_data, labels) as numpy arrays.
    """
    epochs_path = os.path.join(data_dir, f"{subject_id}{session}_epochs.npy")
    labels_path = os.path.join(data_dir, f"{subject_id}{session}_labels.npy")
    
    if not os.path.exists(epochs_path):
        raise FileNotFoundError(f"Epochs file not found: {epochs_path}")
        
    if not os.path.exists(labels_path):
        raise FileNotFoundError(f"Labels file not found: {labels_path}")
    
    epochs_data = np.load(epochs_path)
    labels = np.load(labels_path)
    
    # Handle empty data (like evaluation sessions)
    if epochs_data.size == 0 or labels.size == 0:
        return epochs_data, labels
        
    print(f"Loaded {subject_id}{session} data:")
    print(f"  Epochs shape: {epochs_data.shape}")
    print(f"  Labels shape: {labels.shape}")
    print(f"  Unique labels: {np.unique(labels)}")
    
    return epochs_data, labels

def prepare_dataloaders(epochs_data, labels, test_size=0.2, batch_size=32, seed=42):
    """
    Splits data and creates PyTorch DataLoaders.
    
    Args:
        epochs_data (np.ndarray): Epoched EEG data.
        labels (np.ndarray): Labels for each epoch.
        test_size (float): Proportion of data to use for testing.
        batch_size (int): Batch size for DataLoader.
        seed (int): Random seed for reproducibility.
        
    Returns:
        tuple: (train_loader, test_loader, y_test_original)
    """
    print("Preparing data loaders...")
    
    # Handle empty data
    if epochs_data.size == 0 or labels.size == 0:
        print("Warning: Empty data provided. Returning empty DataLoaders.")
        return None, None, None
    
    # Ensure labels are 0-indexed for PyTorch CrossEntropyLoss
    # BCI IV 2a labels are 1-4, PyTorch expects 0-3
    labels_zero_indexed = labels - 1 
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        epochs_data, labels_zero_indexed, test_size=test_size, random_state=seed, stratify=labels_zero_indexed
    )
    
    # Convert to PyTorch tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Long for classification
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long)
    
    # Create datasets
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"Train loader batches: {len(train_loader)}")
    print(f"Test loader batches: {len(test_loader)}")
    return train_loader, test_loader, y_test  # Return y_test for final evaluation

def train_model(model, train_loader, criterion, optimizer, device, epochs=20):
    """
    Trains the TCN model.
    
    Args:
        model (nn.Module): The TCN model to train.
        train_loader (DataLoader): DataLoader for training data.
        criterion (nn.Module): Loss function.
        optimizer (optim.Optimizer): Optimizer.
        device (torch.device): Device to use for training.
        epochs (int): Number of training epochs.
        
    Returns:
        list: Training losses for each epoch.
    """
    print("Starting training...")
    model.train()  # Set model to training mode
    train_losses = []
    
    for epoch in range(epochs):
        total_loss = 0.0
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()  # Clear gradients
            output = model(data)   # Forward pass
            loss = criterion(output, target)  # Calculate loss
            loss.backward()        # Backward pass
            optimizer.step()       # Update weights
            
            total_loss += loss.item()
            
        avg_loss = total_loss / len(train_loader)
        train_losses.append(avg_loss)
        print(f"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}")
        
    return train_losses

def evaluate_model(model, test_loader, device):
    """
    Evaluates the TCN model and prints metrics.
    
    Args:
        model (nn.Module): The trained TCN model.
        test_loader (DataLoader): DataLoader for test data.
        device (torch.device): Device to use for evaluation.
        
    Returns:
        tuple: (accuracy, kappa, all_preds, all_targets)
    """
    print("Starting evaluation...")
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_targets = []
    
    with torch.no_grad():  # Disable gradient computation for evaluation
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            # Get the predicted class index
            pred = output.argmax(dim=1, keepdim=True)  # shape: (batch_size, 1)
            all_preds.extend(pred.cpu().numpy().flatten())  # Move to CPU and flatten
            all_targets.extend(target.cpu().numpy())  # Move to CPU
            
    # Calculate metrics
    accuracy = accuracy_score(all_targets, all_preds)
    kappa = cohen_kappa_score(all_targets, all_preds)
    
    print(f"Evaluation Results:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Cohen's Kappa: {kappa:.4f}")
    
    return accuracy, kappa, all_preds, all_targets

def save_model(model, filepath):
    """
    Save the trained model to a file.
    
    Args:
        model (nn.Module): The trained model.
        filepath (str): Path to save the model.
    """
    torch.save(model.state_dict(), filepath)
    print(f"Model saved to: {filepath}")

def load_model(model, filepath, device):
    """
    Load a trained model from a file.
    
    Args:
        model (nn.Module): The model architecture.
        filepath (str): Path to the saved model.
        device (torch.device): Device to load the model on.
        
    Returns:
        nn.Module: The loaded model.
    """
    model.load_state_dict(torch.load(filepath, map_location=device))
    print(f"Model loaded from: {filepath}")
    return model

def run_tcn_experiment(subject_id, epochs=10, batch_size=32, learning_rate=0.001, 
                       tcn_output_dim=64, num_channels=[32, 32, 32, 32], kernel_size=3, 
                       dropout=0.2, save_model_flag=False):
    """
    Run a complete TCN experiment for a single subject.
    
    Args:
        subject_id (str): Subject identifier (e.g., 'A01').
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.
        learning_rate (float): Learning rate for optimizer.
        tcn_output_dim (int): Dimension of the TCN output feature vector.
        num_channels (list): List of channel sizes for TCN layers.
        kernel_size (int): Kernel size for TCN convolutions.
        dropout (float): Dropout probability.
        save_model_flag (bool): Whether to save the trained model.
        
    Returns:
        dict: Experiment results.
    """
    print(f"Initializing TCN experiment for subject {subject_id}...")
    
    # Create results directory
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # Check for GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    try:
        # 1. Load data
        epochs_data, labels = load_preprocessed_data(subject_id, 'T', DATA_DIR)
        
        # Handle empty data
        if epochs_data.size == 0 or labels.size == 0:
            print(f"No training data found for subject {subject_id}")
            return None
        
        # 2. Prepare data loaders
        train_loader, test_loader, y_test_original = prepare_dataloaders(
            epochs_data, labels, test_size=0.2, batch_size=batch_size
        )
        
        # Handle empty loaders
        if train_loader is None or test_loader is None:
            print(f"Failed to create data loaders for subject {subject_id}")
            return None
        
        # 3. Define model, loss, and optimizer
        input_size = 22  # Number of EEG channels for BCI IV 2a
        
        model = TCN(input_size=input_size, output_size=tcn_output_dim,
                    num_channels=num_channels, kernel_size=kernel_size, dropout=dropout).to(device)
        
        # Final classifier takes f_tcn as input
        # For 4-class classification
        classifier = nn.Linear(tcn_output_dim, 4).to(device) 
        
        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        # Optimizer for both TCN and classifier
        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), 
                              lr=learning_rate)
        
        print(f"TCN Model instantiated.")
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params_cls = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
        print(f"Trainable parameters in TCN: {total_params}")
        print(f"Trainable parameters in Classifier: {total_params_cls}")
        print(f"Total trainable parameters: {total_params + total_params_cls}")
        
        # 4. Train the model
        train_losses = train_model(model, train_loader, criterion, optimizer, device, epochs)
        
        # 5. Evaluate the model
        # We need to pass the model and the final layer together for evaluation
        # Let's create a combined model for evaluation
        class CombinedModel(nn.Module):
            def __init__(self, tcn, classifier):
                super(CombinedModel, self).__init__()
                self.tcn = tcn
                self.classifier = classifier
                
            def forward(self, x):
                f_tcn = self.tcn(x)
                out = self.classifier(f_tcn)
                return out
                
        combined_model = CombinedModel(model, classifier)
        accuracy, kappa, preds, targets = evaluate_model(combined_model, test_loader, device)
        
        # 6. Save model if requested
        if save_model_flag:
            model_filename = f"tcn_model_{subject_id}.pth"
            model_filepath = os.path.join(RESULTS_DIR, model_filename)
            save_model(model, model_filepath)
            
            classifier_filename = f"classifier_{subject_id}.pth"
            classifier_filepath = os.path.join(RESULTS_DIR, classifier_filename)
            save_model(classifier, classifier_filepath)
        
        print("TCN experiment completed.")
        
        # Return results
        return {
            'subject_id': subject_id,
            'accuracy': accuracy,
            'kappa': kappa,
            'train_losses': train_losses,
            'predictions': preds,
            'targets': targets
        }
        
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    parser = argparse.ArgumentParser(description="Train and evaluate TCN model for BCI.")
    parser.add_argument('--subject_id', type=str, default='A01', 
                        help='Subject ID (e.g., A01)')
    parser.add_argument('--epochs', type=int, default=10, 
                        help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32, 
                        help='Batch size for training')
    parser.add_argument('--learning_rate', type=float, default=0.001, 
                        help='Learning rate for optimizer')
    parser.add_argument('--tcn_output_dim', type=int, default=64, 
                        help='Dimension of TCN output feature vector')
    parser.add_argument('--save_model', action='store_true', 
                        help='Save the trained model')
    
    args = parser.parse_args()
    
    # Run experiment
    results = run_tcn_experiment(
        subject_id=args.subject_id,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        tcn_output_dim=args.tcn_output_dim,
        save_model_flag=args.save_model
    )
    
    if results:
        print("\nExperiment Results:")
        print(f"Subject: {results['subject_id']}")
        print(f"Accuracy: {results['accuracy']:.4f}")
        print(f"Cohen's Kappa: {results['kappa']:.4f}")
    else:
        print("Experiment failed.")

if __name__ == '__main__':
    main()